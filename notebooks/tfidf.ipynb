{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n",
    "    vectorize_texts, SparseFeaturesDataset\n",
    "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Казнить нельзя помиловать Нельзя наказывать','Казнить нельзя помиловать Нельзя освободить', \n",
    "          'Нельзя не помиловать', 'Обязательно освободить']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = lambda x: x.lower().split()\n",
    "corpus_tokens = list(map(clean, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['казнить', 'нельзя', 'помиловать', 'нельзя', 'наказывать'],\n",
       " ['казнить', 'нельзя', 'помиловать', 'нельзя', 'освободить'],\n",
       " ['нельзя', 'не', 'помиловать'],\n",
       " ['обязательно', 'освободить']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, word_doc_freq = build_vocabulary(corpus_tokens, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(zip(vocabulary.keys(), list(word_doc_freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_nums = dict(zip(vocab.keys(),[0,1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2       , 0.        , 0.        , 0.2       , 0.        ,\n",
       "        0.4       , 0.2       ],\n",
       "       [0.        , 0.        , 0.        , 0.2       , 0.2       ,\n",
       "        0.4       , 0.2       ],\n",
       "       [0.        , 0.33333334, 0.        , 0.        , 0.        ,\n",
       "        0.33333334, 0.33333334],\n",
       "       [0.        , 0.        , 0.5       , 0.        , 0.5       ,\n",
       "        0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_texts(corpus_tokens, vocab_nums, vocab.values(), mode='tf', scale=False).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'int' and 'dict_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-5d1cf015ce6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorize_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_nums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'idf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/try python/stepik-dl-nlp/dlnlputils/data/bag_of_words.py\u001b[0m in \u001b[0;36mvectorize_texts\u001b[0;34m(tokenized_texts, word2id, word2freq, mode, scale)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# но оставляем информацию о частотности слова в корпусе в целом\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'idf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mword2freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# учитываем всю информацию, которая у нас есть:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'dict_values'"
     ]
    }
   ],
   "source": [
    "vectorize_texts(corpus_tokens, vocab_nums, vocab.values(), mode='idf', scale=False).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "def tf_idf(tokenized_texts, word2id, word2freq):\n",
    "    \n",
    "    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
    "    for text_i, text in enumerate(tokenized_texts):\n",
    "        for token in text:\n",
    "            if token in word2id:\n",
    "                result[text_i, word2id[token]] += 1\n",
    "                \n",
    "     # получаем вектора относительных частот слова в документе для tf\n",
    "    result_tf = result.tocsr()\n",
    "    result_tf = result_tf.multiply(1 / result.sum(1)).toarray()\n",
    "\n",
    "    # полностью убираем информацию о количестве употреблений слова в данном документе,\n",
    "    # но оставляем информацию о частотности слова в корпусе в целом для idf\n",
    "    result_idf = result.tocsr()\n",
    "    result_idf = (result_idf>0).astype('float32').multiply(1 / np.array(list(word2freq))).toarray()\n",
    "    \n",
    "    matrix = np.log1p(result_tf)*result_idf\n",
    "    std = np.std(matrix, axis=0, ddof=1)\n",
    "    mean = np.mean(matrix, axis=0)\n",
    "    \n",
    "    for i in range(matrix.shape[1]):\n",
    "        matrix[:,i] = (matrix[:,i] - mean[i])/std[i]\n",
    "    \n",
    "    print(np.std(matrix, axis=0, ddof=1))\n",
    "    print(np.mean(matrix, axis=0))\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.        1.        1.        1.        1.        0.9999999 0.9999999]\n",
      "[ 0.0000000e+00  0.0000000e+00 -5.9604645e-08  0.0000000e+00\n",
      "  0.0000000e+00  5.9604645e-08  0.0000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.5       , -0.5       , -0.50000006,  0.8660254 , -0.75312984,\n",
       "         0.809803  ,  0.1428922 ],\n",
       "       [-0.5       , -0.5       , -0.50000006,  0.8660254 ,  0.14848864,\n",
       "         0.809803  ,  0.1428922 ],\n",
       "       [-0.5       ,  1.5       , -0.50000006, -0.8660254 , -0.75312984,\n",
       "        -0.37570813,  1.0650662 ],\n",
       "       [-0.5       , -0.5       ,  1.5       , -0.8660254 ,  1.357771  ,\n",
       "        -1.2438977 , -1.3508506 ]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf(corpus_tokens, vocab_nums, list(vocab.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'наказывать': 0.25,\n",
       " 'не': 0.25,\n",
       " 'обязательно': 0.25,\n",
       " 'казнить': 0.5,\n",
       " 'освободить': 0.5,\n",
       " 'нельзя': 0.75,\n",
       " 'помиловать': 0.75}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
